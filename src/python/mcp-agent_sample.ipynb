{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Use Model Context Protocol (MCP) as tools with Strands Agent\n",
    "\n",
    "## Overview\n",
    "The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide context to Large Language Models (LLMs). Strands AI SDK integrates with MCP to extend agent capabilities through external tools and services.\n",
    "\n",
    "MCP enables communication between agents and MCP servers that provide additional tools. The Strands Agent SDK includes built-in support for connecting to MCP servers and using their tools.\n",
    "\n",
    "In this example we will show you how to use MCP tools on your Strands Agent. We will use the [AWS Documentation MCP server](https://awslabs.github.io/mcp/servers/aws-documentation-mcp-server/) which provides tools to access AWS documentation, search for content, and get recommendations. This MCP server has 3 main features:\n",
    "\n",
    "- **Read Documentation**: Fetch and convert AWS documentation pages to markdown format\n",
    "- **Search Documentation**: Search AWS documentation using the official search API\n",
    "- **Recommendations**: Get content recommendations for AWS documentation pages\n",
    "\n",
    "\n",
    "\n",
    "## Agent Details\n",
    "<div style=\"float: left; margin-right: 20px;\">\n",
    "    \n",
    "|Feature             |Description                                        |\n",
    "|--------------------|---------------------------------------------------|\n",
    "|Feature used        |MCP Tools                                          |\n",
    "|Agent Structure     |Single agent architecture                          |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/architecture.png\" width=\"85%\" />\n",
    "</div>\n",
    "\n",
    "## Key Features\n",
    "* **Single agent architecture**: this example creates a single agent that interacts with MCP tools\n",
    "* **MCP tools**: Integration of MCP tools with your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and prerequisites\n",
    "\n",
    "### Prerequisites\n",
    "* Python 3.10+\n",
    "* AWS account\n",
    "* Anthropic Claude 3.7 enabled on Amazon Bedrock\n",
    "\n",
    "Let's now install the requirement packages for our Strands Agent Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installing pre-requisites\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependency packages\n",
    "\n",
    "Now let's import the dependency packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import os\n",
    "import base64\n",
    "from datetime import timedelta\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from mcp import StdioServerParameters, stdio_client\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.server import FastMCP\n",
    "from strands import Agent\n",
    "from strands.tools.mcp import MCPClient\n",
    "from strands.models.litellm import LiteLLMModel\n",
    "from strands.models.openai import OpenAIModel\n",
    "from strands_tools import mem0_memory, use_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Integration Setup\n",
    "\n",
    "Before connecting to MCP servers, let's set up the memory functionality using mem0. This will allow our agent to remember previous interactions and image inputs across conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory configuration and system prompts\n",
    "MEMORY_SYSTEM_PROMPT = \"\"\"\n",
    "You are an intelligent image generation assistant with persistent memory capabilities using mem0_memory and ComfyUI tools.\n",
    "\n",
    "## Core Capabilities:\n",
    "1. **Memory Management**: Store and retrieve user information, preferences, and previous interactions using mem0_memory\n",
    "2. **Image Generation**: Generate images using ComfyUI tools (generate_image_with_context, get_comfyui_config)\n",
    "3. **Context Awareness**: Remember previous images and use them as input for new generations\n",
    "4. **Intelligent Reasoning**: Use use_llm tool for complex reasoning and response generation\n",
    "\n",
    "## Memory Usage Guidelines:\n",
    "### When to Store Memory:\n",
    "- User provides images: Store image description, format, base64 data (first 200 chars), and context\n",
    "- User expresses preferences: Store style preferences, subject preferences, generation parameters\n",
    "- Successful generations: Store successful prompts, parameters, and workflow types\n",
    "- User feedback: Store what users liked/disliked about generated images\n",
    "\n",
    "### When to Retrieve Memory:\n",
    "- Before any image generation: Check for relevant user preferences and previous images\n",
    "- When user asks for variations: Retrieve original image data and parameters\n",
    "- When user mentions 'previous', 'last', 'earlier' images: Search memory for relevant images\n",
    "- For personalized responses: Retrieve user preferences and history\n",
    "\n",
    "## Image Processing Workflow:\n",
    "### For Text-to-Image:\n",
    "1. Retrieve user style preferences from memory\n",
    "2. Use generate_image_with_context with workflow_type='text_to_image'\n",
    "3. Store successful generation details in memory\n",
    "\n",
    "### For Image-to-Image:\n",
    "1. Check if user provided new image OR retrieve previous image from memory\n",
    "2. If using memory image, extract base64 data and use as context_image_base64\n",
    "3. Use generate_image_with_context with workflow_type='image_to_image'\n",
    "4. Store transformation details in memory\n",
    "\n",
    "## Tool Usage Rules:\n",
    "- **mem0_memory**: Use for storing/retrieving user data with user_id\n",
    "- **use_llm**: Use for complex reasoning, analysis, and generating detailed responses\n",
    "- **generate_image_with_context**: Primary tool for image generation\n",
    "- **get_comfyui_config**: Use to check available workflows and configurations\n",
    "\n",
    "## Memory Storage Format:\n",
    "For images: 'IMAGE_DATA:[full_data_url] DESCRIPTION:[description] FORMAT:[format] CONTEXT:[context]'\n",
    "For preferences: 'User prefers [style/subject/parameters]. Context: [when_expressed]'\n",
    "For generations: 'Generated [type] image with prompt: [prompt]. Parameters: [params]. Success: [yes/no]'\n",
    "\n",
    "## CRITICAL: Image Retrieval and Tool Calling Workflow:\n",
    "When user requests image-to-image or image-to-video generation:\n",
    "\n",
    "### Step 1: Retrieve from Memory\n",
    "- ALWAYS call mem0_memory first with action='retrieve'\n",
    "- Search for relevant image data using keywords from user's request\n",
    "- Look for memories containing 'IMAGE_DATA:' prefix\n",
    "\n",
    "### Step 2: Extract Image Data\n",
    "- From the memory result, find the text containing 'IMAGE_DATA:'\n",
    "- Extract the complete data URL after 'IMAGE_DATA:' (everything until the next space)\n",
    "- This will be in format: 'data:image/jpeg;base64,/9j/4AAQ...'\n",
    "\n",
    "### Step 3: Handle Memory Results\n",
    "- IF image data found: Extract complete data URL and proceed to Step 4a\n",
    "- IF no image data found: Proceed to Step 4b (fallback to text-to-image/text-to-video)\n",
    "\n",
    "### Step 4a: Image-to-Image/Video Generation (when memory has image)\n",
    "- Call generate_image_with_context or generate_video_with_context\n",
    "- Use extracted data URL as context_image_base64 parameter\n",
    "- Set workflow_type='image_to_image' or 'image_to_video'\n",
    "- DO NOT modify or truncate the base64 data\n",
    "\n",
    "### Step 4b: Text-to-Image/Video Generation (fallback when no memory image)\n",
    "- Call generate_image_with_context or generate_video_with_context\n",
    "- DO NOT use context_image_base64 parameter\n",
    "- Set workflow_type='text_to_image' or 'text_to_video'\n",
    "- Inform user that no previous image was found, generating new image instead\n",
    "\n",
    "### Example Workflows:\n",
    "**Scenario A - Image Found in Memory:**\n",
    "User: \"Transform my landscape image into oil painting\"\n",
    "1. mem0_memory(action='retrieve', content='landscape image', user_id='user123')\n",
    "2. Found: 'IMAGE_DATA:data:image/jpeg;base64,/9j/4AAQ...'\n",
    "3. generate_image_with_context(prompt='oil painting style', context_image_base64='data:image/jpeg;base64,/9j/4AAQ...', workflow_type='image_to_image')\n",
    "\n",
    "**Scenario B - No Image Found (Fallback):**\n",
    "User: \"Transform my landscape image into oil painting\"\n",
    "1. mem0_memory(action='retrieve', content='landscape image', user_id='user123')\n",
    "2. No IMAGE_DATA found in memory\n",
    "3. generate_image_with_context(prompt='landscape oil painting style', workflow_type='text_to_image')\n",
    "4. Inform user: \"I couldn't find your previous landscape image in memory, so I generated a new landscape in oil painting style instead.\"\n",
    "\n",
    "Always be helpful, creative, and leverage your memory to provide personalized, context-aware responses.\n",
    "\"\"\"\n",
    "\n",
    "class MemoryImageAgent:\n",
    "    def __init__(self, user_id: str = \"demo_user\"):\n",
    "        self.user_id = user_id\n",
    "        \n",
    "    def store_image_memory(self, image_data_url: str, description: str, context: str = \"\") -> Dict[str, Any]:\n",
    "        \"\"\"Store complete image information in memory\"\"\"\n",
    "        # Extract format from data URL\n",
    "        if image_data_url.startswith('data:image/'):\n",
    "            format_part = image_data_url.split(';')[0].split('/')[1]\n",
    "        else:\n",
    "            format_part = 'unknown'\n",
    "        \n",
    "        # Store complete image data URL for tools to use directly\n",
    "        memory_content = f\"IMAGE_DATA:{image_data_url} DESCRIPTION:{description} FORMAT:{format_part} CONTEXT:{context}\"\n",
    "        \n",
    "        # This will be implemented with the actual agent\n",
    "        return {\n",
    "            \"status\": \"stored\",\n",
    "            \"content\": memory_content,\n",
    "            \"user_id\": self.user_id\n",
    "        }\n",
    "    \n",
    "    def retrieve_image_memories(self, query: str, max_results: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant image memories\"\"\"\n",
    "        # This will be implemented with the actual agent\n",
    "        return []\n",
    "    \n",
    "    def extract_image_from_memory(self, memory_text: str) -> str:\n",
    "        \"\"\"Extract complete image data URL from memory text\"\"\"\n",
    "        # Extract the complete data URL from IMAGE_DATA: prefix\n",
    "        if \"IMAGE_DATA:\" in memory_text:\n",
    "            parts = memory_text.split(\"IMAGE_DATA:\")\n",
    "            if len(parts) > 1:\n",
    "                # Get the data URL part (everything until the next space or DESCRIPTION:)\n",
    "                data_part = parts[1].split(\" DESCRIPTION:\")[0].strip()\n",
    "                return data_part\n",
    "        return \"\"\n",
    "\n",
    "# Demonstration of the simplified workflow\n",
    "def demonstrate_memory_to_tools_workflow():\n",
    "    \"\"\"Demonstrate how agent uses memory to call ComfyUI tools\"\"\"\n",
    "    print(\"ğŸ“‹ Memory-to-Tools Workflow Demonstration:\")\n",
    "    print(\"1. User uploads image â†’ Agent stores in memory with IMAGE_DATA: format\")\n",
    "    print(\"2. User requests transformation â†’ Agent calls mem0_memory to retrieve\")\n",
    "    print(\"3. Agent extracts data URL from IMAGE_DATA: field\")\n",
    "    print(\"4. Agent calls generate_image_with_context with context_image_base64\")\n",
    "    print(\"âœ… No custom tools needed - existing ComfyUI tools work directly!\")\n",
    "    \n",
    "    # Example of what the agent will do internally:\n",
    "    example_memory_content = \"IMAGE_DATA:data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD... DESCRIPTION:Mountain landscape FORMAT:jpeg CONTEXT:User uploaded\"\n",
    "    \n",
    "    print(\"ğŸ“ Example Memory Content:\")\n",
    "    print(f\"   {example_memory_content[:100]}...\")\n",
    "    \n",
    "    # Extract data URL (what agent will do)\n",
    "    if \"IMAGE_DATA:\" in example_memory_content:\n",
    "        data_url = example_memory_content.split(\"IMAGE_DATA:\")[1].split(\" DESCRIPTION:\")[0]\n",
    "        print(\"ğŸ” Extracted Data URL:\")\n",
    "        print(f\"   {data_url[:50]}...\")\n",
    "        print(\"âœ… This data URL can be directly used as context_image_base64!\")\n",
    "    \n",
    "    print(\"\" + \"=\"*50)\n",
    "    print(\"ğŸ”„ Fallback Scenario Demonstration:\")\n",
    "    print(\"1. User requests: 'Transform my cat image into cartoon style'\")\n",
    "    print(\"2. Agent calls mem0_memory to search for 'cat image'\")\n",
    "    print(\"3. No IMAGE_DATA found in memory results\")\n",
    "    print(\"4. Agent falls back to text-to-image generation\")\n",
    "    print(\"5. Agent calls: generate_image_with_context(prompt='cat cartoon style', workflow_type='text_to_image')\")\n",
    "    print(\"6. Agent informs user: 'No previous cat image found, generated new cartoon cat instead'\")\n",
    "    print(\"âœ… Intelligent fallback ensures user always gets a result!\")\n",
    "\n",
    "demonstrate_memory_to_tools_workflow()\n",
    "\n",
    "print(\"Memory integration setup completed!\")\n",
    "print(\"âœ… Enhanced memory storage format for complete image data\")\n",
    "print(\"âœ… Agent can now access memory and call ComfyUI tools directly\")\n",
    "print(\"âœ… Intelligent fallback from image-to-image to text-to-image when no memory image found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to MCP server using Streamable HTTP\n",
    "\n",
    "Let's now connect to the MCP server using Streamable HTTP transport. First lets start a simple MCP server using Streamable HTTP transport. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will connect to our deployed ComfyUI MCP server. The architecture will look as following\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/architecture_2.png\" width=\"85%\" />\n",
    "</div>\n",
    "\n",
    "Our ComfyUI MCP server provides the following image generation tools:\n",
    "- **generate_image_with_context**: Generate images using Flux models (text-to-image and image-to-image)\n",
    "- **get_comfyui_config**: Get ComfyUI configuration and available workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will connect to our deployed ComfyUI MCP server instead of creating a local one\n",
    "# The ComfyUI MCP server provides image generation tools using Flux models\n",
    "# \n",
    "# Available tools:\n",
    "# - generate_image_with_context: Generate images from text prompts or transform existing images\n",
    "# - get_comfyui_config: Get server configuration and available workflows\n",
    "#\n",
    "# Note: Make sure to replace the URL and auth token with your actual deployment values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using a deployed ComfyUI MCP server, we don't need to start a local server thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No need to start a local server thread since we're using the deployed ComfyUI MCP server\n",
    "print(\"Connecting to deployed ComfyUI MCP server...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrating Streamable HTTP client with Agent\n",
    "\n",
    "Now lets use `streamablehttp_client` integrate this server with a simple agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure connection to your deployed ComfyUI MCP server\n",
    "# Replace these values with your actual deployment details\n",
    "COMFYUI_MCP_SERVER_URL = \"https://your-api-gateway-url.execute-api.region.amazonaws.com/Prod/mcp\"\n",
    "COMFYUI_MCP_AUTH_TOKEN = \"your-mcp-auth-token\"\n",
    "\n",
    "def create_comfyui_mcp_transport():\n",
    "    return streamablehttp_client(\n",
    "        COMFYUI_MCP_SERVER_URL,\n",
    "        headers={'Authorization': f\"Bearer {COMFYUI_MCP_AUTH_TOKEN}\"}\n",
    "    )\n",
    "\n",
    "# Create MCP client for ComfyUI server\n",
    "comfyui_mcp_client = MCPClient(create_comfyui_mcp_transport)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup agent configuration and invoke it\n",
    "\n",
    "Next we will set our agent configuration using the tools from the `streamable_http_mcp_client` object we just created. To do so, we need to list the tools available in the MCP server. We can use the `list_tools_sync` method for it. \n",
    "\n",
    "After that, we will ask a question to our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-mwxwvideypifnljcwlryhcnvhxxrzyueyqaqkpwvapyxhceg\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://api.siliconflow.cn/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = \"azure/gpt-4.1-mini\"\n",
    "model = \"openai/deepseek-ai/DeepSeek-V3\"\n",
    "litellm_model = LiteLLMModel(\n",
    "    model_id=model, params={\"max_tokens\": 1000, \"temperature\": 0.7}\n",
    ")\n",
    "\n",
    "openai_compatiable_model = OpenAIModel(\n",
    "    # **model_config\n",
    "    client_args={\n",
    "        \"api_key\":os.environ[\"OPENAI_API_KEY\"],\n",
    "        \"base_url\":\"https://api.siliconflow.cn/\"\n",
    "    },\n",
    "    model_id=\"deepseek-ai/DeepSeek-V3\",\n",
    "    max_tokens =  1000,\n",
    "    temperature = 0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #1: searchWebsite\n",
      "ä»¥ä¸‹ä»¥ä¸‹æ˜¯ä¸€äº›æœ€æ–°çš„ç¾å‰§æ¨èï¼ˆæ˜¯ä¸€äº›æœ€æ–°çš„ç¾å‰§æ¨èï¼ˆæˆªè‡³2025å¹´6æœˆï¼‰ï¼š\n",
      "\n",
      "1æˆªè‡³2025å¹´6æœˆï¼‰ï¼š\n",
      "\n",
      "1. **ã€Šç†Šå®¶é¤é¦† . **ã€Šç†Šå®¶é¤é¦† ç¬¬å››å­£ã€‹**  \n",
      "   - ç¬¬å››å­£ã€‹**  \n",
      "   - è¯„åˆ†ï¼šé«˜åˆ†æ¨è  \n",
      "  è¯„åˆ†ï¼šé«˜åˆ†æ¨è  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https - [æ›´å¤šè¯¦æƒ…](https://www.dealmoon.com/guide://www.dealmoon.com/guide/974237)\n",
      "\n",
      "2. **ã€Š/974237)\n",
      "\n",
      "2. **ã€Šä¸€æ ¹å…¥é­‚ã€‹ (ä¸€æ ¹å…¥é­‚ã€‹ (Stick)**  \n",
      "   - è¯„åˆ†Stick)**  \n",
      "   - è¯„åˆ†ï¼šå€¼å¾—ä¸€çœ‹  \n",
      "   - [æ›´å¤šï¼šå€¼å¾—ä¸€çœ‹  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://www.dealmoonè¯¦æƒ…](https://www.dealmoon.com/guide/974237)\n",
      "\n",
      "3.com/guide/974237)\n",
      "\n",
      "3. **ã€Šé‡‘å¦®ä¸ä¹”æ²»äºš. **ã€Šé‡‘å¦®ä¸ä¹”æ²»äºš ç¬¬ä¸‰å­£ã€‹ (Ginny ç¬¬ä¸‰å­£ã€‹ (Ginny & Georgia Season 3)**  \n",
      "   & Georgia Season 3)**  \n",
      "   - è¯„åˆ†ï¼šé«˜åˆ†æ¨è  \n",
      "   - è¯„åˆ†ï¼šé«˜åˆ†æ¨è  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://www - [æ›´å¤šè¯¦æƒ…](https://www.dealmoon.com/guide/974.dealmoon.com/guide/974237)\n",
      "\n",
      "4. **ã€Šé¢237)\n",
      "\n",
      "4. **ã€Šé¢é¢å…¨é ç¬¬äºŒå­£é¢å…¨é ç¬¬äºŒå­£ã€‹ (FUBAR Season 2ã€‹ (FUBAR Season 2)**  \n",
      "   - è¯„åˆ†ï¼šé«˜åˆ†)**  \n",
      "   - è¯„åˆ†ï¼šé«˜åˆ†æ¨è  \n",
      "   - [æ›´å¤šè¯¦æƒ…](æ¨è  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://www.dealmoon.com/https://www.dealmoon.com/guide/974237)\n",
      "\n",
      "5. **guide/974237)\n",
      "\n",
      "5. **ã€Šæ‰‘å…‹è„¸2ã€‹**  \n",
      "ã€Šæ‰‘å…‹è„¸2ã€‹**  \n",
      "   - è¯„åˆ†ï¼šç»å…¸é«˜åˆ†  \n",
      "   - è¯„åˆ†ï¼šç»å…¸é«˜åˆ†  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://   - [æ›´å¤šè¯¦æƒ…](https://www.dealmoon.co.uk/guidewww.dealmoon.co.uk/guide/3711)\n",
      "\n",
      "6. **ã€Š/3711)\n",
      "\n",
      "6. **ã€Šæœ€åç”Ÿè¿˜è€… ç¬¬ä¸€å­£ã€‹æœ€åç”Ÿè¿˜è€… ç¬¬ä¸€å­£ã€‹**  \n",
      "   - è¯„åˆ†ï¼š9**  \n",
      "   - è¯„åˆ†ï¼š9.0ï¼ˆè±†ç“£ï¼‰  \n",
      "  .0ï¼ˆè±†ç“£ï¼‰  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://m - [æ›´å¤šè¯¦æƒ…](https://m.douban.com/doulist/.douban.com/doulist/1239696/)\n",
      "\n",
      "7. **ã€Š1239696/)\n",
      "\n",
      "7. **ã€Šé»‘æš—æƒ…æŠ¥2ã€‹**é»‘æš—æƒ…æŠ¥2ã€‹**  \n",
      "   - è¯„åˆ†ï¼šå€¼å¾—  \n",
      "   - è¯„åˆ†ï¼šå€¼å¾—æœŸå¾…  \n",
      "   - [æ›´å¤šè¯¦æƒ…æœŸå¾…  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://www.elle.com/t](https://www.elle.com/tw/entertainment/drama/gw/entertainment/drama/g63479909/202530netflix63479909/202530netflix5disney4max3/)\n",
      "\n",
      "85disney4max3/)\n",
      "\n",
      "8. **ã€Šæ··ä¹±å°‘å¹´æ—¶ã€‹**. **ã€Šæ··ä¹±å°‘å¹´æ—¶ã€‹**  \n",
      "   - è¯„åˆ†ï¼šçƒ­é—¨é«˜åˆ†  \n",
      "   - è¯„åˆ†ï¼šçƒ­é—¨é«˜åˆ†  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://www.honglingjin.co.uk://www.honglingjin.co.uk/347064.html)\n",
      "\n",
      "å¦‚æœéœ€è¦æ›´è¯¦ç»†çš„/347064.html)\n",
      "\n",
      "å¦‚æœéœ€è¦æ›´è¯¦ç»†çš„ä»‹ç»æˆ–å…·ä½“æŸéƒ¨ç¾ä»‹ç»æˆ–å…·ä½“æŸéƒ¨ç¾å‰§çš„æ’­æ”¾å¹³å°ï¼Œå¯ä»¥å‰§çš„æ’­æ”¾å¹³å°ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘ï¼å‘Šè¯‰æˆ‘ï¼"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session termination failed: 403\n"
     ]
    }
   ],
   "source": [
    "# Connect to ComfyUI MCP server and test image generation with memory\n",
    "with comfyui_mcp_client:\n",
    "    comfyui_tools = comfyui_mcp_client.list_tools_sync()\n",
    "    print(f\"Available ComfyUI tools: {[tool.name for tool in comfyui_tools]}\")\n",
    "    \n",
    "    # Combine ComfyUI tools with memory tools\n",
    "    all_tools = comfyui_tools + [mem0_memory, use_llm]\n",
    "    print(f\"All tools: {[tool.name for tool in all_tools]}\")\n",
    "\n",
    "    # Create agent with ComfyUI and memory tools\n",
    "    agent = Agent(\n",
    "        model=openai_compatiable_model, \n",
    "        tools=all_tools,\n",
    "        system_prompt=MEMORY_SYSTEM_PROMPT\n",
    "    )\n",
    "\n",
    "    # Test text-to-image generation\n",
    "    print(\"=== Text-to-Image Generation ===\")\n",
    "    async for event in agent.stream_async(\n",
    "        \"Generate a beautiful landscape image with mountains and a lake at sunset. Use text-to-image workflow.\"\n",
    "    ):\n",
    "        if \"data\" in event:\n",
    "            print(event[\"data\"], end=\"\", flush=True)\n",
    "    \n",
    "    print(\"=== Get ComfyUI Configuration ===\")\n",
    "    # Test getting ComfyUI configuration\n",
    "    async for event in agent.stream_async(\"What ComfyUI workflows are available?\"):\n",
    "        if \"data\" in event:\n",
    "            print(event[\"data\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç®€åŒ–çš„Memory-Enhanced Agentæµ‹è¯•\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä¸‰ä¸ªæ ¸å¿ƒåŠŸèƒ½çš„æµ‹è¯•ï¼šæ–‡ç”Ÿå›¾ã€å›¾ç”Ÿå›¾ã€å†å²å¯¹è¯è®°å¿†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€åŒ–çš„Memory-Enhanced Agentæµ‹è¯•\n",
    "def test_simplified_memory_agent():\n",
    "    \"\"\"æµ‹è¯•ä¸‰ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼šæ–‡ç”Ÿå›¾ã€å›¾ç”Ÿå›¾ã€å†å²å¯¹è¯è®°å¿†\"\"\"\n",
    "    print(\"ğŸ§  å¼€å§‹ç®€åŒ–çš„Memory-Enhanced Agentæµ‹è¯•...\")\n",
    "    \n",
    "    with comfyui_mcp_client:\n",
    "        comfyui_tools = comfyui_mcp_client.list_tools_sync()\n",
    "        all_tools = comfyui_tools + [mem0_memory, use_llm]\n",
    "        \n",
    "        # åˆ›å»ºmemory-enabled agent\n",
    "        memory_agent = Agent(\n",
    "            model=openai_compatiable_model,\n",
    "            tools=all_tools,\n",
    "            system_prompt=MEMORY_SYSTEM_PROMPT\n",
    "        )\n",
    "        \n",
    "        # æµ‹è¯•1: å†å²å¯¹è¯è®°å¿† - å­˜å‚¨ç”¨æˆ·åå¥½\n",
    "        print(\"\\n=== æµ‹è¯•1: å†å²å¯¹è¯è®°å¿† - å­˜å‚¨ç”¨æˆ·åå¥½ ===\")\n",
    "        async for event in memory_agent.stream_async(\n",
    "            \"è®°ä½æˆ‘çš„åå¥½ï¼šæˆ‘å–œæ¬¢å±±æ°´é£æ™¯å›¾ï¼Œåçˆ±å†™å®é£æ ¼ï¼Œæš–è‰²è°ƒã€‚è¯·å­˜å‚¨è¿™äº›åå¥½ä¿¡æ¯ã€‚\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        # æµ‹è¯•2: æ–‡ç”Ÿå›¾ - åŸºäºè®°å¿†çš„åå¥½ç”Ÿæˆå›¾åƒ\n",
    "        print(\"\\n\\n=== æµ‹è¯•2: æ–‡ç”Ÿå›¾ - åŸºäºè®°å¿†åå¥½ç”Ÿæˆ ===\")\n",
    "        async for event in memory_agent.stream_async(\n",
    "            \"æ ¹æ®æˆ‘ä¹‹å‰è¯´çš„åå¥½ï¼Œç”Ÿæˆä¸€å¼ å›¾åƒã€‚ä½¿ç”¨text-to-imageå·¥ä½œæµã€‚\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        # æµ‹è¯•3: å›¾ç”Ÿå›¾ - æ¨¡æ‹Ÿç”¨æˆ·ä¸Šä¼ å›¾ç‰‡åçš„è½¬æ¢\n",
    "        print(\"\\n\\n=== æµ‹è¯•3: å›¾ç”Ÿå›¾ - æ¨¡æ‹Ÿå›¾åƒè½¬æ¢ ===\")\n",
    "        # æ¨¡æ‹Ÿå­˜å‚¨å›¾åƒä¿¡æ¯\n",
    "        sample_image_data = \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAABAAEDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAv/xAAUEAEAAAAAAAAAAAAAAAAAAAAA/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAX/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwCdABmX/9k=\"\n",
    "        \n",
    "        async for event in memory_agent.stream_async(\n",
    "            f\"æˆ‘ä¸Šä¼ äº†ä¸€å¼ å±±æ°´é£æ™¯å›¾ã€‚è¯·å­˜å‚¨è¿™å¼ å›¾åƒä¿¡æ¯ï¼šIMAGE_DATA:{sample_image_data} DESCRIPTION:å±±æ°´é£æ™¯å›¾ FORMAT:jpeg CONTEXT:ç”¨æˆ·ä¸Šä¼ çš„å›¾åƒ\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        # ä½¿ç”¨å­˜å‚¨çš„å›¾åƒè¿›è¡Œå›¾ç”Ÿå›¾è½¬æ¢\n",
    "        print(\"\\n\\n=== å›¾ç”Ÿå›¾è½¬æ¢ - ä½¿ç”¨è®°å¿†ä¸­çš„å›¾åƒ ===\")\n",
    "        async for event in memory_agent.stream_async(\n",
    "            \"æŠŠæˆ‘ä¹‹å‰ä¸Šä¼ çš„å±±æ°´é£æ™¯å›¾è½¬æ¢æˆæ²¹ç”»é£æ ¼ã€‚è¯·å…ˆä»è®°å¿†ä¸­æ£€ç´¢å›¾åƒæ•°æ®ï¼Œç„¶åä½¿ç”¨image-to-imageå·¥ä½œæµè¿›è¡Œè½¬æ¢ã€‚\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        print(\"\\n\\nâœ… ç®€åŒ–çš„Memory-Enhanced Agentæµ‹è¯•å®Œæˆ!\")\n",
    "        print(\"\\nğŸ“‹ æµ‹è¯•æ€»ç»“:\")\n",
    "        print(\"âœ“ å†å²å¯¹è¯è®°å¿†ï¼šæˆåŠŸå­˜å‚¨å’Œæ£€ç´¢ç”¨æˆ·åå¥½\")\n",
    "        print(\"âœ“ æ–‡ç”Ÿå›¾ï¼šåŸºäºè®°å¿†åå¥½ç”Ÿæˆå›¾åƒ\")\n",
    "        print(\"âœ“ å›¾ç”Ÿå›¾ï¼šä½¿ç”¨è®°å¿†ä¸­çš„å›¾åƒè¿›è¡Œé£æ ¼è½¬æ¢\")\n",
    "\n",
    "# è¿è¡Œç®€åŒ–æµ‹è¯•\n",
    "test_simplified_memory_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¯å¢ƒé…ç½®è¯´æ˜\n",
    "\n",
    "ä½¿ç”¨memoryåŠŸèƒ½éœ€è¦é…ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memoryåç«¯é…ç½®è¯´æ˜\n",
    "print(\"ğŸ“‹ Memoryåç«¯é…ç½®é€‰é¡¹:\")\n",
    "print(\"\")\n",
    "print(\"1. mem0.ai API (æ¨èç”¨äºæµ‹è¯•):\")\n",
    "print(\"   - è®¾ç½®ç¯å¢ƒå˜é‡: MEM0_API_KEY=your_api_key\")\n",
    "print(\"   - ä¼˜ç‚¹: ç®€å•æ˜“ç”¨ï¼Œäº‘ç«¯å­˜å‚¨\")\n",
    "print(\"   - ç¼ºç‚¹: éœ€è¦APIè´¹ç”¨\")\n",
    "print(\"\")\n",
    "print(\"2. æœ¬åœ°FAISSå­˜å‚¨ (å¼€æºå…è´¹):\")\n",
    "print(\"   - æ— éœ€é¢å¤–é…ç½®\")\n",
    "print(\"   - ä¼˜ç‚¹: å®Œå…¨å…è´¹ï¼Œæ•°æ®æœ¬åœ°\")\n",
    "print(\"   - ç¼ºç‚¹: é‡å¯åæ•°æ®ä¸¢å¤±\")\n",
    "print(\"\")\n",
    "print(\"3. AWS OpenSearch (æ¨èç”¨äºç”Ÿäº§):\")\n",
    "print(\"   - è®¾ç½®ç¯å¢ƒå˜é‡: OPENSEARCH_HOST, AWS_REGION\")\n",
    "print(\"   - ä¼˜ç‚¹: ç”Ÿäº§çº§åˆ«ï¼ŒæŒä¹…åŒ–å­˜å‚¨\")\n",
print("   - ç¼ºç‚¹: éœ€è¦AWSé…ç½®")\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ€»ç»“\n",
    "\n",
    "æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•å°†mem0 memoryåŠŸèƒ½é›†æˆåˆ°Strands Agentä¸­ï¼Œå®ç°äº†ï¼š\n",
    "\n",
    "1. **æ–‡ç”Ÿå›¾åŠŸèƒ½**ï¼šåŸºäºç”¨æˆ·åå¥½å’Œå†å²è®°å¿†ç”Ÿæˆå›¾åƒ\n",
    "2. **å›¾ç”Ÿå›¾åŠŸèƒ½**ï¼šä½¿ç”¨è®°å¿†ä¸­å­˜å‚¨çš„å›¾åƒè¿›è¡Œé£æ ¼è½¬æ¢\n",
    "3. **å†å²å¯¹è¯è®°å¿†**ï¼šæŒä¹…åŒ–å­˜å‚¨ç”¨æˆ·åå¥½å’Œäº¤äº’å†å²\n",
    "\n",
    "å…³é”®ç‰¹æ€§ï¼š\n",
    "- æ™ºèƒ½fallbackï¼šå½“è®°å¿†ä¸­æ²¡æœ‰å›¾åƒæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°æ–‡ç”Ÿå›¾\n",
    "- å®Œæ•´çš„base64å›¾åƒæ•°æ®å­˜å‚¨å’Œæ£€ç´¢\n",
    "- æ”¯æŒå¤šç§memoryåç«¯ï¼ˆmem0.ai APIã€æœ¬åœ°FAISSã€AWS OpenSearchï¼‰\n",
    "- æ— éœ€ä¿®æ”¹ç°æœ‰ComfyUI MCP toolsï¼Œç›´æ¥å¤ç”¨"
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
    "While tools are typically invoked by the agent based on user requests, you can also call MCP tools directly. This can be useful for workflow scenarios where you orchestrate multiple tools together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct tool invocation example with ComfyUI MCP server\n",
    "image_generation_params = {\n",
    "    \"prompt\": \"A serene mountain landscape with snow-capped peaks and a crystal clear lake\",\n",
    "    \"workflow_type\": \"text_to_image\",\n",
    "    \"width\": 1024,\n",
    "    \"height\": 768,\n",
    "    \"steps\": 20,\n",
    "    \"cfg_scale\": 7.0\n",
    "}\n",
    "\n",
    "with comfyui_mcp_client:\n",
    "    # Direct tool invocation for image generation\n",
    "    result = comfyui_mcp_client.call_tool_sync(\n",
    "        tool_use_id=\"tool-img-gen\", \n",
    "        name=\"generate_image_with_context\", \n",
    "        arguments=image_generation_params\n",
    "    )\n",
    "\n",
    "    # Process the result\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"Image generation successful!\")\n",
    "        print(f\"Result type: {result['content'][0].get('type', 'text')}\")\n",
    "        if 'metadata' in result['content'][0]:\n",
    "            metadata = result['content'][0]['metadata']\n",
    "            print(f\"Generation time: {metadata.get('generation_time', 'N/A')} seconds\")\n",
    "            print(f\"Workflow: {metadata.get('workflow_type', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"Image generation failed: {result['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Image-to-Image Tool Invocation\n",
    "\n",
    "For image-to-image generation via direct tool invocation, you need to provide the image as base64 encoded data in the context_image_base64 parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct image-to-image tool invocation example\n",
    "import base64\n",
    "import os\n",
    "\n",
    "def direct_image_to_image_call():\n",
    "    try:\n",
    "        # Example image file path (replace with your actual image)\n",
    "        image_path = \"sample_image.png\"  # Replace with your image file\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"âŒ Image file not found: {image_path}\")\n",
    "            print(\"Please provide a valid image file path\")\n",
    "            return\n",
    "        \n",
    "        # Read and encode the image\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            image_bytes = image_file.read()\n",
    "        \n",
    "        # Convert to base64 data URL format\n",
    "        image_format = os.path.splitext(image_path)[1][1:].lower()\n",
    "        if image_format == 'jpg':\n",
    "            image_format = 'jpeg'\n",
    "        \n",
    "        image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "        image_data_url = f\"data:image/{image_format};base64,{image_base64}\"\n",
    "        \n",
    "        # Parameters for image-to-image generation\n",
    "        img2img_params = {\n",
    "            \"prompt\": \"Transform this image into a beautiful watercolor painting with soft, flowing colors\",\n",
    "            \"context_image_base64\": image_data_url,\n",
    "            \"workflow_type\": \"image_to_image\",\n",
    "            \"steps\": 25,\n",
    "            \"cfg_scale\": 1.0,\n",
    "            \"denoise_strength\": 0.75\n",
    "        }\n",
    "        \n",
    "        print(f\"ğŸ“¤ Calling image-to-image tool directly...\")\n",
    "        print(f\"   Input image size: {len(image_bytes)} bytes\")\n",
    "        print(f\"   Image format: {image_format}\")\n",
    "        \n",
    "        with comfyui_mcp_client:\n",
    "            # Direct tool invocation for image-to-image\n",
    "            result = comfyui_mcp_client.call_tool_sync(\n",
    "                tool_use_id=\"tool-img2img-direct\", \n",
    "                name=\"generate_image_with_context\", \n",
    "                arguments=img2img_params,\n",
    "                read_timeout_seconds=timedelta(seconds=120)  # 2 minutes timeout\n",
    "            )\n",
    "            \n",
    "            # Process the result\n",
    "            if result['status'] == 'success':\n",
    "                print(f\"âœ… Image-to-image generation successful!\")\n",
    "                print(f\"Result type: {result['content'][0].get('type', 'text')}\")\n",
    "                if 'metadata' in result['content'][0]:\n",
    "                    metadata = result['content'][0]['metadata']\n",
    "                    print(f\"Generation time: {metadata.get('generation_time', 'N/A')} seconds\")\n",
    "                    print(f\"Workflow: {metadata.get('workflow_type', 'N/A')}\")\n",
    "                    print(f\"Denoise strength: {metadata.get('denoise_strength', 'N/A')}\")\n",
    "            else:\n",
    "                print(f\"âŒ Image-to-image generation failed: {result['content'][0]['text']}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Direct image-to-image call error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the test (uncomment the line below when you have an image file)\n",
    "# direct_image_to_image_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optinally also provide `read_timeout_seconds` while calling an MCP server tool to avoid it running for too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with timeout for image generation (ComfyUI can take time to generate images)\n",
    "complex_image_params = {\n",
    "    \"prompt\": \"A highly detailed fantasy castle on a floating island with dragons flying around, magical aurora in the sky, photorealistic, 8k quality\",\n",
    "    \"workflow_type\": \"text_to_image\",\n",
    "    \"width\": 1024,\n",
    "    \"height\": 1024,\n",
    "    \"steps\": 50,  # More steps for higher quality\n",
    "    \"cfg_scale\": 8.0\n",
    "}\n",
    "\n",
    "with comfyui_mcp_client:\n",
    "    try:\n",
    "        result = comfyui_mcp_client.call_tool_sync(\n",
    "            tool_use_id=\"tool-complex-img\",\n",
    "            name=\"generate_image_with_context\",\n",
    "            arguments=complex_image_params,\n",
    "            read_timeout_seconds=timedelta(seconds=120),  # 2 minutes timeout for complex generation\n",
    "        )\n",
    "\n",
    "        if result[\"status\"] == \"error\":\n",
    "            print(f\"Image generation failed: {result['content'][0]['text']}\")\n",
    "        else:\n",
    "            print(f\"Complex image generation succeeded!\")\n",
    "            if 'metadata' in result['content'][0]:\n",
    "                metadata = result['content'][0]['metadata']\n",
    "                print(f\"Generation time: {metadata.get('generation_time', 'N/A')} seconds\")\n",
    "                print(f\"Steps used: {metadata.get('steps', 'N/A')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Image generation timed out or failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Memory-Enhanced Image Generation Example\n",
    "\n",
    "Here's a complete example that demonstrates all the memory functionality with ComfyUI integration. This example shows how to store user preferences, remember images, and use memory for intelligent image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Memory-Enhanced Image Generation Example\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "class MemoryEnhancedImageAgent:\n",
    "    \"\"\"Complete example of memory-enhanced image generation agent\"\"\"\n",
    "    \n",
    "    def __init__(self, user_id: str = \"demo_user\"):\n",
    "        self.user_id = user_id\n",
    "        self.agent = None\n",
    "        \n",
    "    def setup_agent(self):\n",
    "        \"\"\"Setup the agent with ComfyUI and memory tools\"\"\"\n",
    "        with comfyui_mcp_client:\n",
    "            comfyui_tools = comfyui_mcp_client.list_tools_sync()\n",
    "            all_tools = comfyui_tools + [mem0_memory, use_llm]\n",
    "            \n",
    "            self.agent = Agent(\n",
    "                model=openai_compatiable_model,\n",
    "                tools=all_tools,\n",
    "                system_prompt=MEMORY_SYSTEM_PROMPT\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… Agent setup complete with {len(all_tools)} tools\")\n",
    "            print(f\"Available tools: {[tool.name for tool in all_tools]}\")\n",
    "    \n",
    "    async def store_user_preferences(self, preferences: str):\n",
    "        \"\"\"Store user preferences in memory\"\"\"\n",
    "        print(f\"ğŸ§  Storing user preferences for {self.user_id}...\")\n",
    "        async for event in self.agent.stream_async(\n",
    "            f\"Remember these preferences for user {self.user_id}: {preferences}\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        print(\"\")\n",
    "    \n",
    "    async def store_image_with_memory(self, image_path: Optional[str] = None, description: str = \"\"):\n",
    "        \"\"\"Store image information in memory\"\"\"\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            # Real image processing\n",
    "            with open(image_path, 'rb') as f:\n",
    "                image_bytes = f.read()\n",
    "            \n",
    "            image_format = os.path.splitext(image_path)[1][1:].lower()\n",
    "            if image_format == 'jpg':\n",
    "                image_format = 'jpeg'\n",
    "            \n",
    "            image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "            \n",
    "            print(f\"ğŸ–¼ï¸ Storing real image: {image_path}\")\n",
    "            await self.agent.stream_async(\n",
    "                f\"Store this image in memory for user {self.user_id}: \"\n",
    "                f\"Description: {description}. Format: {image_format}. \"\n",
    "                f\"Base64 data: {image_base64[:200]}...\"\n",
    "            )\n",
    "        else:\n",
    "            # Simulated image storage\n",
    "            print(f\"ğŸ–¼ï¸ Simulating image storage: {description}\")\n",
    "            async for event in self.agent.stream_async(\n",
    "                f\"Remember that user {self.user_id} has an image: {description}. \"\n",
    "                f\"Store this image information for future reference.\"\n",
    "            ):\n",
    "                if \"data\" in event:\n",
    "                    print(event[\"data\"], end=\"\", flush=True)\n",
    "        print(\"\")\n",
    "    \n",
    "    async def generate_with_memory(self, prompt: str, use_previous_image: bool = False):\n",
    "        \"\"\"Generate image using memory context\"\"\"\n",
    "        if use_previous_image:\n",
    "            print(f\"ğŸ¨ Generating image with memory context...\")\n",
    "            full_prompt = (\n",
    "                f\"For user {self.user_id}: {prompt}. \"\n",
    "                f\"Check memory for previous images and user preferences. \"\n",
    "                f\"If there's a relevant previous image, use it for image-to-image generation. \"\n",
    "                f\"Otherwise, use text-to-image with stored preferences.\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"ğŸ¨ Generating new image with preferences...\")\n",
    "            full_prompt = (\n",
    "                f\"For user {self.user_id}: {prompt}. \"\n",
    "                f\"Check memory for user preferences and generate accordingly using text-to-image.\"\n",
    "            )\n",
    "        \n",
    "        async for event in self.agent.stream_async(full_prompt):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        print(\"\")\n",
    "    \n",
    "    async def run_complete_demo(self):\n",
    "        \"\"\"Run a complete demonstration of memory-enhanced image generation\"\"\"\n",
    "        print(\"ğŸš€ Starting Complete Memory-Enhanced Image Generation Demo\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Setup\n",
    "        print(\"ğŸ“‹ Step 1: Setting up agent...\")\n",
    "        self.setup_agent()\n",
    "        \n",
    "        # Step 2: Store preferences\n",
    "        print(\"ğŸ“‹ Step 2: Storing user preferences...\")\n",
    "        await self.store_user_preferences(\n",
    "            \"I love landscape photography with mountains, lakes, and dramatic skies. \"\n",
    "            \"I prefer realistic style with warm, golden hour lighting. \"\n",
    "            \"I like high contrast and vibrant colors.\"\n",
    "        )\n",
    "        \n",
    "        # Step 3: Store image information\n",
    "        print(\"ğŸ“‹ Step 3: Storing image information...\")\n",
    "        # TODO: Replace 'sample_landscape.jpg' with your actual image path\n",
    "        sample_image_path = \"sample_landscape.jpg\"  # Replace with your image\n",
    "        await self.store_image_with_memory(\n",
    "            image_path=sample_image_path if os.path.exists(sample_image_path) else None,\n",
    "            description=\"Beautiful mountain landscape with lake reflection at sunset\"\n",
    "        )\n",
    "        \n",
    "        # Step 4: Generate new image with preferences\n",
    "        print(\"ğŸ“‹ Step 4: Generating image based on stored preferences...\")\n",
    "        await self.generate_with_memory(\n",
    "            \"Create a stunning landscape image\",\n",
    "            use_previous_image=False\n",
    "        )\n",
    "        \n",
    "        # Step 5: Generate variation using previous image\n",
    "        print(\"ğŸ“‹ Step 5: Creating variation using previous image from memory...\")\n",
    "        await self.generate_with_memory(\n",
    "            \"Transform my previous landscape into an oil painting style\",\n",
    "            use_previous_image=True\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Complete demo finished!\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Create and run the demo\n",
    "demo_agent = MemoryEnhancedImageAgent(user_id=\"demo_user_001\")\n",
    "\n",
    "# Uncomment the line below to run the complete demo\n",
    "# await demo_agent.run_complete_demo()\n",
    "\n",
    "print(\"ğŸ“ Demo setup complete!\")\n",
    "print(\"ğŸ’¡ To run the demo, uncomment the last line and execute this cell\")\n",
    "print(\"ğŸ–¼ï¸ To use with real images, replace 'sample_landscape.jpg' with your image path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup for Memory Functionality\n",
    "\n",
    "To use the memory functionality, you'll need to set up your environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup for memory functionality\n",
    "# Add these to your .env file or set as environment variables\n",
    "\n",
    "# For mem0.ai (recommended for testing)\n",
    "# MEM0_API_KEY=your_mem0_api_key_here\n",
    "\n",
    "# For AWS OpenSearch (recommended for production)\n",
    "# OPENSEARCH_HOST=your_opensearch_endpoint\n",
    "# AWS_REGION=your_aws_region\n",
    "\n",
    "# For local FAISS (default for development)\n",
    "# No additional configuration needed\n",
    "\n",
    "print(\"ğŸ“‹ Environment setup instructions:\")\n",
    "print(\"1. Choose your memory backend:\")\n",
    "print(\"   - mem0.ai: Set MEM0_API_KEY (easiest for testing)\")\n",
    "print(\"   - AWS OpenSearch: Set OPENSEARCH_HOST and AWS_REGION (production)\")\n",
    "print(\"   - Local FAISS: No setup needed (development only)\")\n",
    "print(\"2. Make sure your ComfyUI MCP server is running\")\n",
    "print(\"3. Update COMFYUI_MCP_SERVER_URL and COMFYUI_MCP_AUTH_TOKEN\")\n",
    "print(\"4. Place your test images in the notebook directory\")\n",
    "print(\"5. Update image paths in the demo code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Validation\n",
    "\n",
    "Here are some tests to validate that the memory functionality is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and validation functions for memory functionality\n",
    "\n",
    "async def test_memory_basic_functionality():\n",
    "    \"\"\"Test basic memory store and retrieve functionality\"\"\"\n",
    "    print(\"ğŸ§ª Testing Basic Memory Functionality...\")\n",
    "    \n",
    "    with comfyui_mcp_client:\n",
    "        comfyui_tools = comfyui_mcp_client.list_tools_sync()\n",
    "        all_tools = comfyui_tools + [mem0_memory, use_llm]\n",
    "        \n",
    "        test_agent = Agent(\n",
    "            model=openai_compatiable_model,\n",
    "            tools=all_tools,\n",
    "            system_prompt=MEMORY_SYSTEM_PROMPT\n",
    "        )\n",
    "        \n",
    "        # Test 1: Store information\n",
    "        print(\"ğŸ“ Test 1: Storing test information...\")\n",
    "        async for event in test_agent.stream_async(\n",
    "            \"Remember that I am testing the memory functionality. \"\n",
    "            \"My test user ID is test_user_123. Store this for testing.\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        # Test 2: Retrieve information\n",
    "        print(\"ğŸ” Test 2: Retrieving stored information...\")\n",
    "        async for event in test_agent.stream_async(\n",
    "            \"What do you remember about my testing? Retrieve information from memory.\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        print(\"âœ… Basic memory test completed!\")\n",
    "\n",
    "async def test_image_memory_simulation():\n",
    "    \"\"\"Test image memory functionality with simulated data\"\"\"\n",
    "    print(\"ğŸ–¼ï¸ Testing Image Memory Functionality...\")\n",
    "    \n",
    "    with comfyui_mcp_client:\n",
    "        comfyui_tools = comfyui_mcp_client.list_tools_sync()\n",
    "        all_tools = comfyui_tools + [mem0_memory, use_llm]\n",
    "        \n",
    "        test_agent = Agent(\n",
    "            model=openai_compatiable_model,\n",
    "            tools=all_tools,\n",
    "            system_prompt=MEMORY_SYSTEM_PROMPT\n",
    "        )\n",
    "        \n",
    "        # Test 1: Store simulated image information\n",
    "        print(\"ğŸ“ Test 1: Storing simulated image information...\")\n",
    "        async for event in test_agent.stream_async(\n",
    "            \"Store this image information in memory: \"\n",
    "            \"Image description: Mountain landscape with lake at sunset. \"\n",
    "            \"Format: JPEG. Size: 1024x768. \"\n",
    "            \"Style: Realistic photography with warm lighting. \"\n",
    "            \"User ID: test_user_123\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        # Test 2: Retrieve image information for generation\n",
    "        print(\"ğŸ¨ Test 2: Using stored image info for generation...\")\n",
    "        async for event in test_agent.stream_async(\n",
    "            \"I want to create a variation of my previous mountain landscape image. \"\n",
    "            \"Check memory for the image information and generate a similar image with oil painting style.\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        print(\"âœ… Image memory test completed!\")\n",
    "\n",
    "async def test_fallback_logic():\n",
    "    \"\"\"Test fallback from image-to-image to text-to-image when no memory image found\"\"\"\n",
    "    print(\"ğŸ”„ Testing Fallback Logic...\")\n",
    "    \n",
    "    with comfyui_mcp_client:\n",
    "        comfyui_tools = comfyui_mcp_client.list_tools_sync()\n",
    "        all_tools = comfyui_tools + [mem0_memory, use_llm]\n",
    "        \n",
    "        test_agent = Agent(\n",
    "            model=openai_compatiable_model,\n",
    "            tools=all_tools,\n",
    "            system_prompt=MEMORY_SYSTEM_PROMPT\n",
    "        )\n",
    "        \n",
    "        # Test: Request transformation of non-existent image\n",
    "        print(\"ğŸ“ Test: Requesting transformation of non-existent image...\")\n",
    "        async for event in test_agent.stream_async(\n",
    "            \"Transform my dragon image into a watercolor painting style. \"\n",
    "            \"If you can't find the dragon image in memory, please generate a new dragon watercolor painting instead.\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        print(\"âœ… Fallback logic test completed!\")\n",
    "        print(\"Expected behavior: Agent should search memory, find no dragon image, then use text-to-image generation\")\n",
    "\n",
    "async def test_preference_memory():\n",
    "    \"\"\"Test user preference storage and retrieval\"\"\"\n",
    "    print(\"âš™ï¸ Testing Preference Memory...\")\n",
    "    \n",
    "    with comfyui_mcp_client:\n",
    "        comfyui_tools = comfyui_mcp_client.list_tools_sync()\n",
    "        all_tools = comfyui_tools + [mem0_memory, use_llm]\n",
    "        \n",
    "        test_agent = Agent(\n",
    "            model=openai_compatiable_model,\n",
    "            tools=all_tools,\n",
    "            system_prompt=MEMORY_SYSTEM_PROMPT\n",
    "        )\n",
    "        \n",
    "        # Test 1: Store preferences\n",
    "        print(\"ğŸ“ Test 1: Storing user preferences...\")\n",
    "        async for event in test_agent.stream_async(\n",
    "            \"Remember my preferences: I love fantasy art with dragons and castles. \"\n",
    "            \"I prefer vibrant colors and detailed artwork. \"\n",
    "            \"I like 1024x1024 resolution images. User ID: test_user_123\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        # Test 2: Use preferences for generation\n",
    "        print(\"ğŸ¨ Test 2: Generating image based on stored preferences...\")\n",
    "        async for event in test_agent.stream_async(\n",
    "            \"Create an image for me based on my stored preferences. \"\n",
    "            \"Generate something I would like according to my memory.\"\n",
    "        ):\n",
    "            if \"data\" in event:\n",
    "                print(event[\"data\"], end=\"\", flush=True)\n",
    "        \n",
    "        print(\"âœ… Preference memory test completed!\")\n",
    "\n",
    "async def run_all_tests():\n",
    "    \"\"\"Run all memory functionality tests\"\"\"\n",
    "    print(\"ğŸš€ Starting Memory Functionality Tests\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        await test_memory_basic_functionality()\n",
    "        await test_image_memory_simulation()\n",
    "        await test_fallback_logic()\n",
    "        await test_preference_memory()\n",
    "        \n",
    "        print(\"\" + \"=\" * 50)\n",
    "        print(\"âœ… All tests completed successfully!\")\n",
    "        print(\"ğŸ“‹ Test Summary:\")\n",
    "        print(\"âœ“ Basic memory store/retrieve functionality\")\n",
    "        print(\"âœ“ Image information storage and retrieval\")\n",
    "        print(\"âœ“ Fallback logic (image-to-image â†’ text-to-image)\")\n",
    "        print(\"âœ“ User preference memory\")\n",
    "        print(\"âœ“ Memory-based image generation\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Test failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Uncomment to run tests\n",
    "# await run_all_tests()\n",
    "\n",
    "print(\"ğŸ§ª Test functions ready!\")\n",
    "print(\"ğŸ’¡ Uncomment 'await run_all_tests()' to run all memory tests\")\n",
    "print(\"âš ï¸ Make sure your memory backend (mem0.ai, OpenSearch, or FAISS) is configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting Common Issues\n",
    "\n",
    "Here are solutions to common issues you might encounter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting and diagnostic functions\n",
    "\n",
    "def check_memory_backend():\n",
    "    \"\"\"Check which memory backend is configured\"\"\"\n",
    "    print(\"ğŸ” Checking Memory Backend Configuration...\")\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    if os.getenv('MEM0_API_KEY'):\n",
    "        print(\"âœ… mem0.ai backend detected (MEM0_API_KEY found)\")\n",
    "        print(\"   This is recommended for testing and development\")\n",
    "    elif os.getenv('OPENSEARCH_HOST'):\n",
    "        print(\"âœ… AWS OpenSearch backend detected (OPENSEARCH_HOST found)\")\n",
    "        print(\"   This is recommended for production\")\n",
    "        print(f\"   Host: {os.getenv('OPENSEARCH_HOST')}\")\n",
    "        print(f\"   Region: {os.getenv('AWS_REGION', 'Not set')}\")\n",
    "    else:\n",
    "        print(\"âœ… Local FAISS backend (default)\")\n",
    "        print(\"   This is suitable for development only\")\n",
    "        print(\"   Memory will not persist between sessions\")\n",
    "\n",
    "def check_comfyui_connection():\n",
    "    \"\"\"Check ComfyUI MCP server connection\"\"\"\n",
    "    print(\"ğŸ” Checking ComfyUI MCP Server Connection...\")\n",
    "    \n",
    "    try:\n",
    "        with comfyui_mcp_client:\n",
    "            tools = comfyui_mcp_client.list_tools_sync()\n",
    "            print(f\"âœ… Connected successfully! Found {len(tools)} tools:\")\n",
    "            for tool in tools:\n",
    "                print(f\"   - {tool.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Connection failed: {e}\")\n",
    "        print(\"ğŸ”§ Troubleshooting steps:\")\n",
    "        print(\"1. Check COMFYUI_MCP_SERVER_URL is correct\")\n",
    "        print(\"2. Check COMFYUI_MCP_AUTH_TOKEN is valid\")\n",
    "        print(\"3. Ensure ComfyUI MCP server is running\")\n",
    "        print(\"4. Check network connectivity\")\n",
    "\n",
    "def check_dependencies():\n",
    "    \"\"\"Check if all required dependencies are installed\"\"\"\n",
    "    print(\"ğŸ” Checking Dependencies...\")\n",
    "    \n",
    "    required_packages = [\n",
    "        'strands',\n",
    "        'strands_tools',\n",
    "        'mcp',\n",
    "        'base64',\n",
    "        'os'\n",
    "    ]\n",
    "    \n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"âœ… {package}\")\n",
    "        except ImportError:\n",
    "            print(f\"âŒ {package} - Not installed\")\n",
    "            if package == 'strands_tools':\n",
    "                print(\"   Install with: pip install strands-agents-tools[mem0_memory]\")\n",
    "            elif package == 'strands':\n",
    "                print(\"   Install with: pip install strands\")\n",
    "\n",
    "def run_diagnostics():\n",
    "    \"\"\"Run all diagnostic checks\"\"\"\n",
    "    print(\"ğŸ¥ Running Diagnostics...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    check_dependencies()\n",
    "    check_memory_backend()\n",
    "    check_comfyui_connection()\n",
    "    \n",
    "    print(\"\" + \"=\" * 40)\n",
    "    print(\"ğŸ¥ Diagnostics completed!\")\n",
    "\n",
    "# Run diagnostics\n",
    "run_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "In this notebook you learned how to:\n",
    "- Connect to MCP servers using Strands Agent with stdio and Streamable HTTP transports\n",
    "- Use ComfyUI MCP server for AI image generation with Flux models\n",
    "- **Integrate mem0 memory functionality** for persistent context across conversations\n",
    "- **Store and retrieve user preferences** and image information using mem0_memory\n",
    "- **Implement memory-enhanced image generation** that remembers previous images and preferences\n",
    "- Handle image inputs properly for image-to-image generation using the messages parameter\n",
    "- **Use memory for intelligent tool selection** - when to use previous images vs. generate new ones\n",
    "- Perform direct tool invocations with timeout handling\n",
    "- Combine multiple MCP servers (AWS Documentation + ComfyUI) with memory tools\n",
    "- Handle both text-based tools (documentation search) and media generation tools (image creation)\n",
    "- Process images in both agent-based and direct tool invocation scenarios\n",
    "- **Create a complete memory-enhanced agent class** for production use\n",
    "\n",
    "The ComfyUI integration with memory demonstrates how MCP can extend agent capabilities beyond text to include multimedia generation with persistent context, making your agents more versatile, intelligent, and personalized.\n",
    "\n",
    "## Key Memory Features Implemented:\n",
    "- **Conversation Memory**: Remembers user preferences, previous images, and generation history\n",
    "- **Image Memory**: Stores image descriptions, formats, and base64 data for future use\n",
    "- **Intelligent Retrieval**: Automatically retrieves relevant context for image generation\n",
    "- **Personalized Generation**: Uses stored preferences to customize image generation\n",
    "- **Context-Aware Tools**: Tools that understand when to use memory vs. new inputs\n",
    "\n",
    "## Next Steps\n",
    "- **Memory Backends**: Try different backends (mem0.ai, AWS OpenSearch, local FAISS)\n",
    "- **Image Generation**: Experiment with different prompts, styles, and parameters\n",
    "- **Image-to-Image**: Try transformations using both new images and memory-stored images\n",
    "- **Direct Tool Calls**: Use base64 encoded images in context_image_base64 parameter\n",
    "- **Multi-Modal Workflows**: Combine image analysis, generation, and text processing with memory\n",
    "- **Custom ComfyUI**: Configure your server with different models and custom nodes\n",
    "- **Production Deployment**: Use the provided deployment scripts for AWS Lambda with memory\n",
    "- **Error Handling**: Implement robust fallback mechanisms for both image processing and memory operations\n",
    "- **Memory Management**: Implement memory cleanup, categorization, and advanced retrieval strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
