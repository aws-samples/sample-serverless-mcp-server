{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Use Model Context Protocol (MCP) as tools with Strands Agent\n",
    "\n",
    "## Overview\n",
    "The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide context to Large Language Models (LLMs). Strands AI SDK integrates with MCP to extend agent capabilities through external tools and services.\n",
    "\n",
    "MCP enables communication between agents and MCP servers that provide additional tools. The Strands Agent SDK includes built-in support for connecting to MCP servers and using their tools.\n",
    "\n",
    "In this example we will show you how to use MCP tools on your Strands Agent. We will use the [AWS Documentation MCP server](https://awslabs.github.io/mcp/servers/aws-documentation-mcp-server/) which provides tools to access AWS documentation, search for content, and get recommendations. This MCP server has 3 main features:\n",
    "\n",
    "- **Read Documentation**: Fetch and convert AWS documentation pages to markdown format\n",
    "- **Search Documentation**: Search AWS documentation using the official search API\n",
    "- **Recommendations**: Get content recommendations for AWS documentation pages\n",
    "\n",
    "\n",
    "\n",
    "## Agent Details\n",
    "<div style=\"float: left; margin-right: 20px;\">\n",
    "    \n",
    "|Feature             |Description                                        |\n",
    "|--------------------|---------------------------------------------------|\n",
    "|Feature used        |MCP Tools                                          |\n",
    "|Agent Structure     |Single agent architecture                          |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/architecture.png\" width=\"85%\" />\n",
    "</div>\n",
    "\n",
    "## Key Features\n",
    "* **Single agent architecture**: this example creates a single agent that interacts with MCP tools\n",
    "* **MCP tools**: Integration of MCP tools with your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and prerequisites\n",
    "\n",
    "### Prerequisites\n",
    "* Python 3.10+\n",
    "* AWS account\n",
    "* Anthropic Claude 3.7 enabled on Amazon Bedrock\n",
    "\n",
    "Let's now install the requirement packages for our Strands Agent Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installing pre-requisites\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependency packages\n",
    "\n",
    "Now let's import the dependency packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "from mcp import StdioServerParameters, stdio_client\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from mcp.server import FastMCP\n",
    "from strands import Agent\n",
    "from strands.tools.mcp import MCPClient\n",
    "from strands.models.litellm import LiteLLMModel\n",
    "from strands.models.openai import OpenAIModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to MCP server using stdio transport\n",
    "\n",
    "[Transposts](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports) in MCP provide the foundations for communication between clients and servers. It handles the underlying mechanics of how messages are sent and received. At the moment there are two standards transport implementations built-in in MCP:\n",
    "\n",
    "- **Standard Input/Output (stdio)**: enables communication through standard input and output streams. It is particulary useful for local integrations and command-line tools\n",
    "- **Streamable HTTP**: this replaces the HTTP+SSSE transport from previous protocol version. In the Streamable HTTP transport, the server operates as an independent process that can handle multiple client connections. This transport uses HTTP POST and GET requests. Server can optionally make use of Server-Sent Events (SSE) to stream multiple server messages. This permits basic MCP servers, as well as more feature-rich servers supporting streaming and server-to-client notifications and requests.\n",
    "\n",
    "Overall, you should use stdio for building command-line tools, implementing local integrations and working with shell scripts. You should use Streamable HTTP transports when you need a flexible and efficient way for AI agents to communicate with tools and services, especially when dealing with stateless communication or when minimizing resource usage is crucial.\n",
    "\n",
    "You can also use custom transports implementation for your specific needs. \n",
    "\n",
    "\n",
    "Let's now connect to the MCP server using stdio transport. First of all, we will use the class `MCPClient` to connect to the [AWS Documentation MCP Server](https://awslabs.github.io/mcp/servers/aws-documentation-mcp-server/). This server provides tools to access AWS documentation, search for content, and get recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to an MCP server using stdio transport\n",
    "stdio_mcp_client = MCPClient(\n",
    "    lambda: stdio_client(\n",
    "        StdioServerParameters(\n",
    "            command=\"uvx\", args=[\"awslabs.aws-documentation-mcp-server@latest\"]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup agent configuration and invoke it\n",
    "\n",
    "Next we will set our agent configuration using the tools from the `stdio_mcp_client` object we just created. To do so, we need to list the tools available in the MCP server. We can use the `list_tools_sync` method for it. \n",
    "\n",
    "After that, we will ask a question to our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with MCP tools\n",
    "with stdio_mcp_client:\n",
    "    # Get the tools from the MCP server\n",
    "    tools = stdio_mcp_client.list_tools_sync()\n",
    "\n",
    "    # Create an agent with these tools\n",
    "    agent = Agent(tools=tools)\n",
    "\n",
    "    response = agent(\"What is Amazon Bedrock pricing model. Be concise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to MCP server using Streamable HTTP\n",
    "\n",
    "Let's now connect to the MCP server using Streamable HTTP transport. First lets start a simple MCP server using Streamable HTTP transport. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will connect to our deployed ComfyUI MCP server. The architecture will look as following\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/architecture_2.png\" width=\"85%\" />\n",
    "</div>\n",
    "\n",
    "Our ComfyUI MCP server provides the following image generation tools:\n",
    "- **generate_image_with_context**: Generate images using Flux models (text-to-image and image-to-image)\n",
    "- **get_comfyui_config**: Get ComfyUI configuration and available workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will connect to our deployed ComfyUI MCP server instead of creating a local one\n",
    "# The ComfyUI MCP server provides image generation tools using Flux models\n",
    "# \n",
    "# Available tools:\n",
    "# - generate_image_with_context: Generate images from text prompts or transform existing images\n",
    "# - get_comfyui_config: Get server configuration and available workflows\n",
    "#\n",
    "# Note: Make sure to replace the URL and auth token with your actual deployment values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using a deployed ComfyUI MCP server, we don't need to start a local server thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No need to start a local server thread since we're using the deployed ComfyUI MCP server\n",
    "print(\"Connecting to deployed ComfyUI MCP server...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrating Streamable HTTP client with Agent\n",
    "\n",
    "Now lets use `streamablehttp_client` integrate this server with a simple agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure connection to your deployed ComfyUI MCP server\n",
    "# Replace these values with your actual deployment details\n",
    "COMFYUI_MCP_SERVER_URL = \"https://your-api-gateway-url.execute-api.region.amazonaws.com/Prod/mcp\"\n",
    "COMFYUI_MCP_AUTH_TOKEN = \"your-mcp-auth-token\"\n",
    "\n",
    "def create_comfyui_mcp_transport():\n",
    "    return streamablehttp_client(\n",
    "        COMFYUI_MCP_SERVER_URL,\n",
    "        headers={'Authorization': f\"Bearer {COMFYUI_MCP_AUTH_TOKEN}\"}\n",
    "    )\n",
    "\n",
    "# Create MCP client for ComfyUI server\n",
    "comfyui_mcp_client = MCPClient(create_comfyui_mcp_transport)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup agent configuration and invoke it\n",
    "\n",
    "Next we will set our agent configuration using the tools from the `streamable_http_mcp_client` object we just created. To do so, we need to list the tools available in the MCP server. We can use the `list_tools_sync` method for it. \n",
    "\n",
    "After that, we will ask a question to our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-mwxwvideypifnljcwlryhcnvhxxrzyueyqaqkpwvapyxhceg\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://api.siliconflow.cn/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = \"azure/gpt-4.1-mini\"\n",
    "model = \"openai/deepseek-ai/DeepSeek-V3\"\n",
    "litellm_model = LiteLLMModel(\n",
    "    model_id=model, params={\"max_tokens\": 1000, \"temperature\": 0.7}\n",
    ")\n",
    "\n",
    "openai_compatiable_model = OpenAIModel(\n",
    "    # **model_config\n",
    "    client_args={\n",
    "        \"api_key\":os.environ[\"OPENAI_API_KEY\"],\n",
    "        \"base_url\":\"https://api.siliconflow.cn/\"\n",
    "    },\n",
    "    model_id=\"deepseek-ai/DeepSeek-V3\",\n",
    "    max_tokens =  1000,\n",
    "    temperature = 0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #1: searchWebsite\n",
      "ä»¥ä¸‹ä»¥ä¸‹æ˜¯ä¸€äº›æœ€æ–°çš„ç¾å‰§æ¨èï¼ˆæ˜¯ä¸€äº›æœ€æ–°çš„ç¾å‰§æ¨èï¼ˆæˆªè‡³2025å¹´6æœˆï¼‰ï¼š\n",
      "\n",
      "1æˆªè‡³2025å¹´6æœˆï¼‰ï¼š\n",
      "\n",
      "1. **ã€Šç†Šå®¶é¤é¦† . **ã€Šç†Šå®¶é¤é¦† ç¬¬å››å­£ã€‹**  \n",
      "   - ç¬¬å››å­£ã€‹**  \n",
      "   - è¯„åˆ†ï¼šé«˜åˆ†æ¨è  \n",
      "  è¯„åˆ†ï¼šé«˜åˆ†æ¨è  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https - [æ›´å¤šè¯¦æƒ…](https://www.dealmoon.com/guide://www.dealmoon.com/guide/974237)\n",
      "\n",
      "2. **ã€Š/974237)\n",
      "\n",
      "2. **ã€Šä¸€æ ¹å…¥é­‚ã€‹ (ä¸€æ ¹å…¥é­‚ã€‹ (Stick)**  \n",
      "   - è¯„åˆ†Stick)**  \n",
      "   - è¯„åˆ†ï¼šå€¼å¾—ä¸€çœ‹  \n",
      "   - [æ›´å¤šï¼šå€¼å¾—ä¸€çœ‹  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://www.dealmoonè¯¦æƒ…](https://www.dealmoon.com/guide/974237)\n",
      "\n",
      "3.com/guide/974237)\n",
      "\n",
      "3. **ã€Šé‡‘å¦®ä¸ä¹”æ²»äºš. **ã€Šé‡‘å¦®ä¸ä¹”æ²»äºš ç¬¬ä¸‰å­£ã€‹ (Ginny ç¬¬ä¸‰å­£ã€‹ (Ginny & Georgia Season 3)**  \n",
      "   & Georgia Season 3)**  \n",
      "   - è¯„åˆ†ï¼šé«˜åˆ†æ¨è  \n",
      "   - è¯„åˆ†ï¼šé«˜åˆ†æ¨è  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://www - [æ›´å¤šè¯¦æƒ…](https://www.dealmoon.com/guide/974.dealmoon.com/guide/974237)\n",
      "\n",
      "4. **ã€Šé¢237)\n",
      "\n",
      "4. **ã€Šé¢é¢å…¨é ç¬¬äºŒå­£é¢å…¨é ç¬¬äºŒå­£ã€‹ (FUBAR Season 2ã€‹ (FUBAR Season 2)**  \n",
      "   - è¯„åˆ†ï¼šé«˜åˆ†)**  \n",
      "   - è¯„åˆ†ï¼šé«˜åˆ†æ¨è  \n",
      "   - [æ›´å¤šè¯¦æƒ…](æ¨è  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://www.dealmoon.com/https://www.dealmoon.com/guide/974237)\n",
      "\n",
      "5. **guide/974237)\n",
      "\n",
      "5. **ã€Šæ‰‘å…‹è„¸2ã€‹**  \n",
      "ã€Šæ‰‘å…‹è„¸2ã€‹**  \n",
      "   - è¯„åˆ†ï¼šç»å…¸é«˜åˆ†  \n",
      "   - è¯„åˆ†ï¼šç»å…¸é«˜åˆ†  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://   - [æ›´å¤šè¯¦æƒ…](https://www.dealmoon.co.uk/guidewww.dealmoon.co.uk/guide/3711)\n",
      "\n",
      "6. **ã€Š/3711)\n",
      "\n",
      "6. **ã€Šæœ€åç”Ÿè¿˜è€… ç¬¬ä¸€å­£ã€‹æœ€åç”Ÿè¿˜è€… ç¬¬ä¸€å­£ã€‹**  \n",
      "   - è¯„åˆ†ï¼š9**  \n",
      "   - è¯„åˆ†ï¼š9.0ï¼ˆè±†ç“£ï¼‰  \n",
      "  .0ï¼ˆè±†ç“£ï¼‰  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://m - [æ›´å¤šè¯¦æƒ…](https://m.douban.com/doulist/.douban.com/doulist/1239696/)\n",
      "\n",
      "7. **ã€Š1239696/)\n",
      "\n",
      "7. **ã€Šé»‘æš—æƒ…æŠ¥2ã€‹**é»‘æš—æƒ…æŠ¥2ã€‹**  \n",
      "   - è¯„åˆ†ï¼šå€¼å¾—  \n",
      "   - è¯„åˆ†ï¼šå€¼å¾—æœŸå¾…  \n",
      "   - [æ›´å¤šè¯¦æƒ…æœŸå¾…  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://www.elle.com/t](https://www.elle.com/tw/entertainment/drama/gw/entertainment/drama/g63479909/202530netflix63479909/202530netflix5disney4max3/)\n",
      "\n",
      "85disney4max3/)\n",
      "\n",
      "8. **ã€Šæ··ä¹±å°‘å¹´æ—¶ã€‹**. **ã€Šæ··ä¹±å°‘å¹´æ—¶ã€‹**  \n",
      "   - è¯„åˆ†ï¼šçƒ­é—¨é«˜åˆ†  \n",
      "   - è¯„åˆ†ï¼šçƒ­é—¨é«˜åˆ†  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https  \n",
      "   - [æ›´å¤šè¯¦æƒ…](https://www.honglingjin.co.uk://www.honglingjin.co.uk/347064.html)\n",
      "\n",
      "å¦‚æœéœ€è¦æ›´è¯¦ç»†çš„/347064.html)\n",
      "\n",
      "å¦‚æœéœ€è¦æ›´è¯¦ç»†çš„ä»‹ç»æˆ–å…·ä½“æŸéƒ¨ç¾ä»‹ç»æˆ–å…·ä½“æŸéƒ¨ç¾å‰§çš„æ’­æ”¾å¹³å°ï¼Œå¯ä»¥å‰§çš„æ’­æ”¾å¹³å°ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘ï¼å‘Šè¯‰æˆ‘ï¼"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session termination failed: 403\n"
     ]
    }
   ],
   "source": [
    "# Connect to ComfyUI MCP server and test image generation\n",
    "with comfyui_mcp_client:\n",
    "    tools = comfyui_mcp_client.list_tools_sync()\n",
    "    print(f\"Available tools: {[tool.name for tool in tools]}\")\n",
    "\n",
    "    # Create agent with ComfyUI tools\n",
    "    agent = Agent(model=openai_compatiable_model, tools=tools)\n",
    "\n",
    "    # Test text-to-image generation\n",
    "    print(\"\\n=== Text-to-Image Generation ===\")\n",
    "    async for event in agent.stream_async(\n",
    "        \"Generate a beautiful landscape image with mountains and a lake at sunset. Use text-to-image workflow.\"\n",
    "    ):\n",
    "        if \"data\" in event:\n",
    "            print(event[\"data\"], end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\n=== Get ComfyUI Configuration ===\")\n",
    "    # Test getting ComfyUI configuration\n",
    "    async for event in agent.stream_async(\"What ComfyUI workflows are available?\"):\n",
    "        if \"data\" in event:\n",
    "            print(event[\"data\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image-to-Image Generation with Agent\\n",
    "\\n",
    "For image-to-image generation, we need to provide an input image to the agent. This demonstrates how to properly handle image inputs in Strands Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-to-Image generation example with proper image input handling\\n",
    "import os\\n",
    "\\n",
    "def test_image_to_image_generation():\\n",
    "    \\\"\\\"\\\"Test image-to-image generation with ComfyUI MCP server\\\"\\\"\\\"\\n",
    "    print(\\\"ğŸ–¼ï¸ Testing Image-to-Image Generation...\\\")\\n",
    "    \\n",
    "    try:\\n",
    "        # Example image file path (replace with your actual image)\\n",
    "        image_path = \\\"sample_image.png\\\"  # Replace with your image file\\n",
    "        \\n",
    "        if not os.path.exists(image_path):\\n",
    "            print(f\\\"âŒ Image file not found: {image_path}\\\")\\n",
    "            print(\\\"Please provide a valid image file path\\\")\\n",
    "            return\\n",
    "        \\n",
    "        # Read the image file\\n",
    "        with open(image_path, 'rb') as image_file:\\n",
    "            image_bytes = image_file.read()\\n",
    "        \\n",
    "        # Get image format\\n",
    "        image_format = os.path.splitext(image_path)[1][1:].lower()\\n",
    "        if image_format == 'jpg':\\n",
    "            image_format = 'jpeg'\\n",
    "        \\n",
    "        # Create messages with image input\\n",
    "        messages = [\\n",
    "            {\\n",
    "                \\\"role\\\": \\\"user\\\",\\n",
    "                \\\"content\\\": [\\n",
    "                    {\\n",
    "                        \\\"image\\\": {\\n",
    "                            \\\"format\\\": image_format,\\n",
    "                            \\\"source\\\": {\\n",
    "                                \\\"bytes\\\": image_bytes\\n",
    "                            }\\n",
    "                        }\\n",
    "                    },\\n",
    "                    {\\n",
    "                        \\\"text\\\": \\\"I have uploaded an image that I want to transform.\\\"\\n",
    "                    }\\n",
    "                ]\\n",
    "            }\\n",
    "        ]\\n",
    "        \\n",
    "        print(f\\\"ğŸ“¤ Creating Agent with image input...\\\")\\n",
    "        print(f\\\"   Image format: {image_format}\\\")\\n",
    "        print(f\\\"   Image size: {len(image_bytes)} bytes\\\")\\n",
    "        \\n",
    "        # Connect to ComfyUI MCP server and create agent with image\\n",
    "        with comfyui_mcp_client:\\n",
    "            tools = comfyui_mcp_client.list_tools_sync()\\n",
    "            \\n",
    "            # Create agent with image messages\\n",
    "            agent = Agent(\\n",
    "                model=openai_compatiable_model, \\n",
    "                tools=tools, \\n",
    "                messages=messages  # Key: pass the image in messages\\n",
    "            )\\n",
    "            \\n",
    "            # Now call the agent with transformation request\\n",
    "            print(\\\"\\\\n=== Image-to-Image Transformation ===\")\\n",
    "            async for event in agent.stream_async(\\n",
    "                \\\"Transform this image into an oil painting style using the image-to-image workflow. \\\"\\n",
    "                \\\"Make it look artistic and painterly with visible brush strokes.\\\"\\n",
    "            ):\\n",
    "                if \\\"data\\\" in event:\\n",
    "                    print(event[\\\"data\\\"], end=\\\"\\\", flush=True)\\n",
    "            \\n",
    "            print(\\\"\\\\nâœ… Image-to-image generation completed!\\\")\\n",
    "            \\n",
    "    except Exception as e:\\n",
    "        print(f\\\"âŒ Image-to-image generation error: {e}\\\")\\n",
    "        import traceback\\n",
    "        traceback.print_exc()\\n",
    "\\n",
    "# Run the test (uncomment the line below when you have an image file)\\n",
    "# test_image_to_image_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Tool Invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While tools are typically invoked by the agent based on user requests, you can also call MCP tools directly. This can be useful for workflow scenarios where you orchestrate multiple tools together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tool execution failed: Session terminated\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/strands/tools/mcp/mcp_client.py\", line 189, in call_tool_sync\n",
      "    call_tool_result: MCPCallToolResult = self._invoke_on_background_thread(_call_tool_async())\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/strands/tools/mcp/mcp_client.py\", line 298, in _invoke_on_background_thread\n",
      "    return future.result()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/strands/tools/mcp/mcp_client.py\", line 186, in _call_tool_async\n",
      "    return await self._background_thread_session.call_tool(name, arguments, read_timeout_seconds)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/mcp/client/session.py\", line 281, in call_tool\n",
      "    return await self.send_request(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/mcp/shared/session.py\", line 294, in send_request\n",
      "    raise McpError(response_or_error.error)\n",
      "mcp.shared.exceptions.McpError: Session terminated\n",
      "Session termination failed: 403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation result: Tool execution failed: Session terminated\n"
     ]
    }
   ],
   "source": [
    "# Direct tool invocation example with ComfyUI MCP server\n",
    "image_generation_params = {\n",
    "    \"prompt\": \"A serene mountain landscape with snow-capped peaks and a crystal clear lake\",\n",
    "    \"workflow_type\": \"text_to_image\",\n",
    "    \"width\": 1024,\n",
    "    \"height\": 768,\n",
    "    \"steps\": 20,\n",
    "    \"cfg_scale\": 7.0\n",
    "}\n",
    "\n",
    "with comfyui_mcp_client:\n",
    "    # Direct tool invocation for image generation\n",
    "    result = comfyui_mcp_client.call_tool_sync(\n",
    "        tool_use_id=\"tool-img-gen\", \n",
    "        name=\"generate_image_with_context\", \n",
    "        arguments=image_generation_params\n",
    "    )\n",
    "\n",
    "    # Process the result\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"Image generation successful!\")\n",
    "        print(f\"Result type: {result['content'][0].get('type', 'text')}\")\n",
    "        if 'metadata' in result['content'][0]:\n",
    "            metadata = result['content'][0]['metadata']\n",
    "            print(f\"Generation time: {metadata.get('generation_time', 'N/A')} seconds\")\n",
    "            print(f\"Workflow: {metadata.get('workflow_type', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"Image generation failed: {result['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Image-to-Image Tool Invocation\\n",
    "\\n",
    "For image-to-image generation via direct tool invocation, you need to provide the image as base64 encoded data in the context_image_base64 parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct image-to-image tool invocation example\\n",
    "import base64\\n",
    "import os\\n",
    "\\n",
    "def direct_image_to_image_call():\\n",
    "    \\\"\\\"\\\"Direct tool invocation for image-to-image generation\\\"\\\"\\\"\\n",
    "    try:\\n",
    "        # Example image file path (replace with your actual image)\\n",
    "        image_path = \\\"sample_image.png\\\"  # Replace with your image file\\n",
    "        \\n",
    "        if not os.path.exists(image_path):\\n",
    "            print(f\\\"âŒ Image file not found: {image_path}\\\")\\n",
    "            print(\\\"Please provide a valid image file path\\\")\\n",
    "            return\\n",
    "        \\n",
    "        # Read and encode the image\\n",
    "        with open(image_path, 'rb') as image_file:\\n",
    "            image_bytes = image_file.read()\\n",
    "        \\n",
    "        # Convert to base64 data URL format\\n",
    "        image_format = os.path.splitext(image_path)[1][1:].lower()\\n",
    "        if image_format == 'jpg':\\n",
    "            image_format = 'jpeg'\\n",
    "        \\n",
    "        image_base64 = base64.b64encode(image_bytes).decode('utf-8')\\n",
    "        image_data_url = f\\\"data:image/{image_format};base64,{image_base64}\\\"\\n",
    "        \\n",
    "        # Parameters for image-to-image generation\\n",
    "        img2img_params = {\\n",
    "            \\\"prompt\\\": \\\"Transform this image into a beautiful watercolor painting with soft, flowing colors\\\",\\n",
    "            \\\"context_image_base64\\\": image_data_url,\\n",
    "            \\\"workflow_type\\\": \\\"image_to_image\\\",\\n",
    "            \\\"steps\\\": 25,\\n",
    "            \\\"cfg_scale\\\": 1.0,\\n",
    "            \\\"denoise_strength\\\": 0.75\\n",
    "        }\\n",
    "        \\n",
    "        print(f\\\"ğŸ“¤ Calling image-to-image tool directly...\\\")\\n",
    "        print(f\\\"   Input image size: {len(image_bytes)} bytes\\\")\\n",
    "        print(f\\\"   Image format: {image_format}\\\")\\n",
    "        \\n",
    "        with comfyui_mcp_client:\\n",
    "            # Direct tool invocation for image-to-image\\n",
    "            result = comfyui_mcp_client.call_tool_sync(\\n",
    "                tool_use_id=\\\"tool-img2img-direct\\\", \\n",
    "                name=\\\"generate_image_with_context\\\", \\n",
    "                arguments=img2img_params,\\n",
    "                read_timeout_seconds=timedelta(seconds=120)  # 2 minutes timeout\\n",
    "            )\\n",
    "            \\n",
    "            # Process the result\\n",
    "            if result['status'] == 'success':\\n",
    "                print(f\\\"âœ… Image-to-image generation successful!\\\")\\n",
    "                print(f\\\"Result type: {result['content'][0].get('type', 'text')}\\\")\\n",
    "                if 'metadata' in result['content'][0]:\\n",
    "                    metadata = result['content'][0]['metadata']\\n",
    "                    print(f\\\"Generation time: {metadata.get('generation_time', 'N/A')} seconds\\\")\\n",
    "                    print(f\\\"Workflow: {metadata.get('workflow_type', 'N/A')}\\\")\\n",
    "                    print(f\\\"Denoise strength: {metadata.get('denoise_strength', 'N/A')}\\\")\\n",
    "            else:\\n",
    "                print(f\\\"âŒ Image-to-image generation failed: {result['content'][0]['text']}\\\")\\n",
    "                \\n",
    "    except Exception as e:\\n",
    "        print(f\\\"âŒ Direct image-to-image call error: {e}\\\")\\n",
    "        import traceback\\n",
    "        traceback.print_exc()\\n",
    "\\n",
    "# Run the test (uncomment the line below when you have an image file)\\n",
    "# direct_image_to_image_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optinally also provide `read_timeout_seconds` while calling an MCP server tool to avoid it running for too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with timeout for image generation (ComfyUI can take time to generate images)\n",
    "complex_image_params = {\n",
    "    \"prompt\": \"A highly detailed fantasy castle on a floating island with dragons flying around, magical aurora in the sky, photorealistic, 8k quality\",\n",
    "    \"workflow_type\": \"text_to_image\",\n",
    "    \"width\": 1024,\n",
    "    \"height\": 1024,\n",
    "    \"steps\": 50,  # More steps for higher quality\n",
    "    \"cfg_scale\": 8.0\n",
    "}\n",
    "\n",
    "with comfyui_mcp_client:\n",
    "    try:\n",
    "        result = comfyui_mcp_client.call_tool_sync(\n",
    "            tool_use_id=\"tool-complex-img\",\n",
    "            name=\"generate_image_with_context\",\n",
    "            arguments=complex_image_params,\n",
    "            read_timeout_seconds=timedelta(seconds=120),  # 2 minutes timeout for complex generation\n",
    "        )\n",
    "\n",
    "        if result[\"status\"] == \"error\":\n",
    "            print(f\"Image generation failed: {result['content'][0]['text']}\")\n",
    "        else:\n",
    "            print(f\"Complex image generation succeeded!\")\n",
    "            if 'metadata' in result['content'][0]:\n",
    "                metadata = result['content'][0]['metadata']\n",
    "                print(f\"Generation time: {metadata.get('generation_time', 'N/A')} seconds\")\n",
    "                print(f\"Steps used: {metadata.get('steps', 'N/A')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Image generation timed out or failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with multiple MCP servers\n",
    "\n",
    "With Strands Agents you can also interact with multiple MCP servers using the same agent and configure tools setups such as the max number of tools that can be used in parallel (`max_parallel_tools`). Let's create a new agent to show case this configuration:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/architecture_3.png\" width=\"85%\" />\n",
    "</div>\n",
    "\n",
    "In this agent, we will use the AWS Documentation MCP server and our ComfyUI MCP server. This demonstrates how to combine different types of tools - documentation search and image generation.\n",
    "\n",
    "First let's connect to the AWS Documentation MCP server using stdio transport and our ComfyUI server using streamable HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AWS Documentation MCP server using stdio transport\n",
    "aws_docs_mcp_client = MCPClient(\n",
    "    lambda: stdio_client(\n",
    "        StdioServerParameters(\n",
    "            command=\"uvx\", args=[\"awslabs.aws-documentation-mcp-server@latest\"]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# We already have our ComfyUI MCP client configured above\n",
    "# comfyui_mcp_client = MCPClient(create_comfyui_mcp_transport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Agent with MCP servers\n",
    "\n",
    "Next we will create the agent with the tools from both MCP servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with tools from both MCP servers\n",
    "with aws_docs_mcp_client, comfyui_mcp_client:\n",
    "    # Get the tools from both MCP servers\n",
    "    aws_tools = aws_docs_mcp_client.list_tools_sync()\n",
    "    comfyui_tools = comfyui_mcp_client.list_tools_sync()\n",
    "    all_tools = aws_tools + comfyui_tools\n",
    "    \n",
    "    print(f\"AWS Documentation tools: {[tool.name for tool in aws_tools]}\")\n",
    "    print(f\"ComfyUI tools: {[tool.name for tool in comfyui_tools]}\")\n",
    "\n",
    "    # Create an agent with these tools\n",
    "    agent = Agent(model=openai_compatiable_model, tools=all_tools, max_parallel_tools=2)\n",
    "\n",
    "    # Test combining documentation search with image generation\n",
    "    print(\"\\n=== Multi-Server Agent Test ===\")\n",
    "    async for event in agent.stream_async(\n",
    "        \"First, search for information about Amazon Bedrock pricing. Then generate an image that represents cloud computing and AI services.\"\n",
    "    ):\n",
    "        if \"data\" in event:\n",
    "            print(event[\"data\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "In this notebook you learned how to:\n",
    "- Connect to MCP servers using Strands Agent with stdio and Streamable HTTP transports\n",
    "- Use ComfyUI MCP server for AI image generation with Flux models\n",
    "- Handle image inputs properly for image-to-image generation using the messages parameter\n",
    "- Perform direct tool invocations with timeout handling\n",
    "- Combine multiple MCP servers (AWS Documentation + ComfyUI) in a single agent\n",
    "- Handle both text-based tools (documentation search) and media generation tools (image creation)\n",
    "- Process images in both agent-based and direct tool invocation scenarios\n",
    "\n",
    "The ComfyUI integration demonstrates how MCP can extend agent capabilities beyond text to include multimedia generation, making your agents more versatile and powerful.\n",
    "\n",
    "## Next Steps\n",
    "- **Image Generation**: Experiment with different prompts, styles, and parameters\n",
    "- **Image-to-Image**: Try transformations by providing images via the messages parameter\n",
    "- **Direct Tool Calls**: Use base64 encoded images in context_image_base64 parameter\n",
    "- **Multi-Modal Workflows**: Combine image analysis, generation, and text processing\n",
    "- **Custom ComfyUI**: Configure your server with different models and custom nodes\n",
    "- **Production Deployment**: Use the provided deployment scripts for AWS Lambda\n",
    "- **Error Handling**: Implement robust fallback mechanisms for image processing failures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
